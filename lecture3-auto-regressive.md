# [MIT 6.S978 Deep Generative Models](https://mit-6s978.github.io/schedule.html)

## 课程笔记与详细解析 (Fall 2024)

# [第三讲：自回归模型](https://mit-6s978.github.io/assets/pdfs/lec3_ar.pdf)

## 目录 (Table of Contents)

- [1. 概述](#1-概述)
- [2. 条件分布建模](#2-条件分布建模)
  - [2.1 从独立变量到依赖变量](#21-从独立变量到依赖变量)
  - [2.2 联合分布的两种解决方案](#22-联合分布的两种解决方案)
  - [2.3 链式法则基础](#23-链式法则基础)
  - [2.4 排序的灵活性](#24-排序的灵活性)
  - [2.5 三个案例研究](#25-三个案例研究)
  - [2.6 使用神经网络建模](#26-使用神经网络建模)
  - [2.7 依赖图](#27-依赖图)
  - [2.8 与深度学习相同的精神："分治"](#28-与深度学习相同的精神分治)
  - [2.9 总结](#29-总结)
- [3. 自回归模型](#3-自回归模型)
  - [3.1 直观示例](#31-直观示例)
  - [3.2 定义：自动 + 回归](#32-定义自动--回归)
  - [3.3 一般公式](#33-一般公式)
  - [3.4 归纳偏置](#34-归纳偏置-inductive-bias)
  - [3.5 表示一个分布](#35-表示一个分布)
  - [3.6 推理：自回归过程](#36-推理自回归过程)
  - [3.7 训练挑战](#37-训练挑战)
  - [3.8 解决方案：教师强制](#38-解决方案教师强制)
  - [3.9 运行示例：MNIST上的AR](#39-运行示例mnist上的ar)
  - [3.10 架构无关性](#310-架构无关性)
  - [3.11 自回归模型总结](#311-自回归模型总结)
- [4. 自回归建模的网络架构](#4-自回归建模的网络架构)
  - [4.1 共享计算挑战](#41-共享计算挑战)
  - [4.2 解决方案：共享计算](#42-解决方案共享计算)
  - [4.3 常见架构概述](#43-常见架构概述)
  - [4.4 递归神经网络（RNN）用于AR](#44-递归神经网络rnn用于ar)
  - [4.5 卷积神经网络（CNN）用于AR](#45-卷积神经网络cnn用于ar)
  - [4.6 Transformer用于AR](#46-transformer用于ar)
  - [4.7 架构比较](#47-架构比较)
  - [4.8 统一框架](#48-统一框架)
- [5. 实例分析](#5-实例分析)
  - [5.1 示例1：MNIST像素生成](#51-示例1mnist像素生成)
  - [5.2 示例2：Char-RNN](#52-示例2char-rnn)
  - [5.3 示例3：WaveNet](#53-示例3wavenet)
  - [5.4 示例4：image GPT（iGPT）](#54-示例4image-gptigpt)
- [6. 总结和关键要点](#6-总结和关键要点)
  - [6.1 主要概念](#61-主要概念)
  - [6.2 关键设计选择](#62-关键设计选择)
  - [6.3 自回归模型的优势](#63-自回归模型的优势)
  - [6.4 挑战和限制](#64-挑战和限制)
  - [6.5 现代发展和未来方向](#65-现代发展和未来方向)
- [参考文献](#参考文献)
  - [主要参考文献](#主要参考文献)
  - [额外参考文献](#额外参考文献)
- [附录：数学基础](#附录数学基础)
  - [A.1 概率的链式法则](#a1-概率的链式法则)
  - [A.2 对数似然目标](#a2-对数似然目标)
  - [A.3 交叉熵损失](#a3-交叉熵损失)
  - [A.4 教师强制与自回归损失](#a4-教师强制与自回归损失)

---

## 1. 概述

本讲涵盖自回归建模中的三个基本主题：

1. **条件分布建模** - 使用链式法则分解的数学基础
2. **自回归模型** - 核心概念、训练策略和归纳偏差
3. **自回归建模的网络架构** - RNN、CNN和Transformer实现

自回归模型代表了生成建模中最成功的范式之一，支撑着GPT、WaveNet和PixelRNN等系统。它们通过将联合分布分解为条件分布的乘积来建模联合分布，实现了易处理的似然计算和灵活的生成。

---

## 2. 条件分布建模

### 2.1 从独立变量到依赖变量

建模联合分布的最简单方法是假设独立性：

$$
p(x_1, x_2) = p(x_1)p(x_2)
$$

这意味着 $p(x_2 \mid x_1) = p(x_2)$- 条件概率等于边际概率。在视觉上，这表示在坐标空间中变量之间不相互影响。

然而，现实世界的问题几乎总是涉及依赖变量，其中：

$$
p(x_1, x_2) \neq p(x_1)p(x_2)
$$

在这种情况下， $p(x_2 \mid x_1) \neq p(x_2) $- 条件分布根据 $x_1 $的值而变化。

### 2.2 联合分布的两种解决方案

**方案1：通过独立潜变量建模（VAE方法）**

- 核心思想：在简单的潜空间学习表示，然后映射到复杂的观察空间
- 在潜空间中假设独立性： $p(z_1, z_2) = p(z_1)p(z_2) $
- 通过解码器映射到观察空间： $x = f_\theta(z) $
- 观察数据 $x $的各维度可以相互依赖，但这种依赖性是通过独立的潜变量 $z $间接产生的
- 限制：
  - 通常需要低维潜在表示（例如用32维 $z $表示784维 $x $）
  - 对高维数据来说独立潜变量假设过于严格（例如：VAE在MNIST上的结果显示生成图像模糊，这是因为独立潜变量难以捕捉784个像素间的复杂依赖关系）
  - 是良好的Building Block，但通常单独使用不够充分

**方案2：通过条件分布建模**

- 这是自回归模型的重点
- 使用链式法则分解联合分布
- 不需要独立性假设

### 2.3 链式法则基础

概率的链式法则表明，任何联合分布都可以写成条件概率的乘积：

对于两个变量：

$$
p(A, B) = p(A)p(B \mid A)
$$

对于三个变量：

$$
p(A, B, C) = p(A)p(B \mid A)p(C \mid A, B)
$$

### 2.4 排序的灵活性

**任何排序都是有效的** - 三个变量的所有6种排列在数学上都是等价的：

1. $p(A, B, C) = p(A)p(B \mid A)p(C \mid A, B) $
2. $p(A, B, C) = p(A)p(C \mid A)p(B \mid A, C) $
3. $p(A, B, C) = p(B)p(A \mid B)p(C \mid A, B) $
4. $p(A, B, C) = p(B)p(C \mid B)p(A \mid B, C) $
5. $p(A, B, C) = p(C)p(A \mid C)p(B \mid A, C) $
6. $p(A, B, C) = p(C)p(B \mid C)p(A \mid B, C) $

**任何分区都是有效的**：

- $p(A, B, C, D) = p(A, B)p(C, D \mid A, B) $
- $p(A, B, C, D) = p(C, D)p(A, B \mid C, D) $
- $p(A, B, C, D) = p(A, B, C)p(D \mid A, B, C) $
- $p(A, B, C, D) = p(B, C)p(A, D \mid B, C) $

**注意**：这种灵活性是 **掩码自回归(MAR)** 模型的基础（Li等，2024）。

### 2.5 三个案例研究

**案例1：分割输入表示空间 $x $**

- 示例：文本标记或像素上的自回归模型
- 直接建模 $p(x_1, x_2, \ldots, x_n) $

**案例2：分割潜在表示空间 $z $**

- 示例：VQ-VAE标记上的自回归模型
- 建模 $p(z_1, z_2, \ldots, z_k) $，其中 $z_i $是离散标记

**案例3：渐进式变换数据分布**

- 示例：扩散模型
- 跨时间步建模 $p(x_{t-1} \mid x_t) $

### 2.6 使用神经网络建模

每个条件分布都可以用神经网络建模：

$$
p(A, B, C) = p_\theta(A) \cdot p_\phi(B \mid A) \cdot p_\psi(C \mid A, B)
$$

**重要考虑**：

- **概念上不同的映射**：每个条件具有不同的输入/输出结构
- **变量与条件数量**：
  - $p(A, B, C) $有3个输出变量需要联合预测
  - $p(C \mid A, B) $有1个输出变量和2个输入条件（成为网络输入）
- **权重共享**：可以在条件间共享架构和权重
- **归纳偏差**：权重共享暗示对条件间相似性的假设

**自回归的降维威力**：

假设 $A $, $B $, $C $每个都有10种可能的取值，我们可以用一个直观的例子展示自回归如何简化问题：

$$
\underbrace{p(A, B, C)}_{\mathrm{1000维输出}} = \underbrace{p(A)}_{\mathrm{10维}} \times \underbrace{p(B \mid A)}_{\mathrm{10维}} \times \underbrace{p(C \mid A,B)}_{\mathrm{10维}}
$$

- **直接建模联合分布**： $p(A, B, C) $需要输出 $10 \times 10 \times 10 = 1000 $个概率值（覆盖所有可能的组合）
- **自回归分解**：将其分解为3个独立的10维分类问题
  - $p(A) $：从10个选项中选择
  - $p(B \mid A) $：给定 $A $，从10个选项中选择 $B $
  - $p(C \mid A, B) $：给定 $A $和 $B $，从10个选项中选择 $C $

**关键洞察**：将一个1000维的问题分解成3个10维的问题！这就是自回归建模的核心力量——通过链式法则将指数级复杂的联合分布问题转化为多个线性复杂度的条件分布问题。

### 2.7 依赖图

分解联合分布会产生反映假设因果结构的**依赖图**。

有些依赖图可能导致比其他图更简单的分布：

- $p(A, B, C) = p(A)p(B \mid A)p(C \mid A, B) $与
- $p(A, B, C) = p(C)p(B \mid C)p(A \mid B, C) $

两者在数学上都是有效的，但基于数据中的真实潜在依赖关系，一种排序可能比另一种更容易学习。

**示例**：如果C自然依赖于A和B，第一种分解与数据生成过程一致，将更容易建模。

### 2.8 与深度学习相同的精神："分治"

这种方法反映了深度学习的基本原理：

- **导数的链式法则**（反向传播）： $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial y} \frac{\partial y}{\partial x} $
- **概率的链式法则**： $p(x, y) = p(x)p(y \mid x) $

两者都将复杂问题分解为更简单、可管理的部分。

### 2.9 总结

关键原理：

- **联合分布 → 条件概率的乘积**：通过链式法则总是可能的
- **链式法则实现分治**：将复杂分布分解为更简单的部分
- **灵活性**：任何排序、任何分区在数学上都是有效的
- **依赖图**：分解的选择编码关于关系的先验知识(即归纳偏置，inductive bias)
- **通用方法**：不特定于自回归模型 - 适用于许多生成范式

---

## 3. 自回归模型

### 3.1 直观示例

**ChatGPT：下一个标记预测**

- 给定"天气是"，预测"晴朗"、"下雨"、"寒冷"等
- 每个预测都依赖于所有前面的标记
- 顺序生成过程

**你的键盘：预测文本**

- 手机键盘根据输入历史建议下一个词
- 前面的上下文显著减少可能的下一个选项
- 实时自回归预测

### 3.2 定义：自动 + 回归

**Auto（自动）**："Self" - 使用模型自己的输出作为后续预测的输入

**Regression（回归）**：估计变量间的关系，建模条件依赖关系 $p(x_i \mid x_{<i}) $

* **传统统计学中的"回归"**：通常指预测连续数值（如房价、温度）
* **自回归中的"回归"**：更广义的概念，指建模任意类型的条件分布
  * **离散情况**：输出是分类分布（如预测下一个词 - 从词汇表中选择）
  * **连续情况**：输出是连续分布（如预测像素值 - 高斯分布或混合Logistic分布）。这里的"连续 vs 离散"指的是**数据类型和分布类型**，与序列性（sequential）无关

推理与训练的区别：

- **"自回归"指推理时行为**：模型使用自己生成的输出
- **训练不一定是自回归的**：通常使用带有真实标签输入的"教师强制"

### 3.3 一般公式

自回归将联合分布建模为条件概率的乘积：

$$
p(x_1, x_2, \ldots, x_n) = p(x_1) \prod_{i=2}^{n} p(x_i \mid x_1, \ldots, x_{i-1})
$$

**关键性质**：

- **x可以是任何表示**：不一定是顺序或时间的
  - 向量的所有维度
  - 2D、3D或高维数组
  - 标记、像素、特征等
- **任何排序和分区**：如条件建模中讨论的
  - 排序：反向、随机、特定领域的
  - 分区：标量、向量或张量块
- **每个条件可以采用任何形式**：查找表、决策树、神经网络或混合
- **无近似**：这种分解是精确的（仅仅是链式法则）

### 3.4 归纳偏置 Inductive Bias

**目标**：选择产生更简单条件分布的分解

**示例1：手机键盘**
前面的输出显著约束下一个可能性：

- 在"天气"后，下一个选项有限（"是"、"预报"）
- 在"天气是"后，选项受限（"好"、"坏"、"晴朗"等）
- 每个条件 $p(x_i \mid x_1, \ldots, x_{i-1}) $比联合建模 $p(x_1, \ldots, x_n) $更容易

**示例2：分类分布复杂性**

- $p(x_1) $："更难" - 许多可能的第一选择
- $p(x_2 \mid x_1) $："更简单" - 受 $x_1 $约束
- $p(x_3 \mid x_1, x_2) $：更受约束
- $p(x_4 \mid x_1, x_2, x_3) $：非常受约束
- $p(x_5 \mid x_1, x_2, x_3, x_4) $：高度受约束

每个条件分布变成更简单的分类问题。

**时间建模偏差**
Next Token Preidiction 暗示时间结构：

- 沿时间轴的自然排序
- 过去影响未来，反之则不然
- 与语言、音频等的因果结构一致

**架构和权重共享**

- **概念上**：每个 $p(x_i \mid x_1, \ldots, x_{i-1}) $是不同的映射
- **实际上**：用共享架构建模它们（RNN、CNN、Transformer）
- **共享权重 $\theta $**：所有条件使用相同参数
- **权衡**：虽然引入了近似（假设所有位置使用相同函数形式），但实现了计算效率和更好的泛化能力

### 3.5 表示一个分布

**网络结构**：

- **输入**： $x_1, \ldots, x_{i-1} $（前面的元素）
- **输出**：在 $x_i $（下一个元素）上的分布

**分布类型**：

- **连续分布**：高斯、高斯混合
- **离散分布**：分类

**注意**：离散分布在AR模型中很受欢迎，因为：

- 容易实现（标准分类）
- 训练稳定
- 但连续分布也是有效的

### 3.6 推理：自回归过程

逐步生成过程：

1. **步骤1**：从 $p(x_1) $采样

   - 网络输入：无（或特殊开始标记）
   - 网络输出：在 $x_1 $上的分布
2. **步骤2**：从 $p(x_2 \mid x_1) $采样

   - 网络输入： $x_1 $（来自步骤1）
   - 网络输出：在 $x_2 $上的分布
3. **步骤3**：从 $p(x_3 \mid x_1, x_2) $采样

   - 网络输入： $x_1, x_2 $（来自前面步骤）
   - 网络输出：在 $x_3 $上的分布
4. **步骤 $n $**：从 $p(x_n \mid x_1, \ldots, x_{n-1}) $采样

   - 网络输入： $x_1, \ldots, x_{n-1} $（来自所有前面步骤）
   - 网络输出：在 $x_n $上的分布

**性质**：

- **递归过程**：每步依赖于所有前面的输出
- **不一定是RNN**：可以用任何架构实现
- **推理时顺序**：必须一次生成一个元素

### 3.7 训练挑战

**朴素反向传播的问题**：

考虑6步序列中 $x_6 $的梯度路径：

- 必须通过 $x_5 $（及其采样操作）反向传播
- 然后通过 $x_4 $（及其采样操作）
- 然后通过 $x_3 $、 $x_2 $、 $x_1 $...
- 每个都涉及完整的神经网络（例如，完整的Transformer）

**问题**：

- 极长的梯度路径
- 采样操作不可微
- 内存需求随序列长度缩放
- 对于实际序列长度**计算不可行**

### 3.8 解决方案：教师强制

**关键洞察**：在训练期间使用真实标签数据作为输入，而不是模型自己的预测。

**训练设置**：

- **输入**：真实标签 $x_1, x_2, \ldots, x_{n-1} $
- **目标**：真实标签 $x_2, x_3, \ldots, x_n $（偏移一个位置）
- **损失**： $\mathcal{L} = -\sum_{i=2}^{n} \log p_\theta(x_i \mid x_1, \ldots, x_{i-1}) $

**优势**：

- **更短的反向传播路径**：每个预测都是独立的
- **并行计算**：可以同时计算所有预测
- **真实标签输入简化训练**：模型看到完美上下文
- **稳定梯度**：梯度路径中没有采样操作

**劣势**：

- **训练/推理不匹配**：不同的输入分布
- **分布偏移**：模型在训练期间从未看到自己的错误
- **暴露偏差**：推理时的累积错误

### 3.9 运行示例：MNIST上的AR

**设置**：

1. 将28×28图像视为784像素的序列
2. 使用光栅扫描顺序（从左到右，从上到下）

**推理过程**：

- **$p(x_1) $**：采样第一个像素（左上角）
- **$p(x_2 \mid x_1) $**：给定第一个像素采样第二个像素
- **$p(x_3 \mid x_1, x_2) $**：给定前两个像素采样第三个像素
- **$p(x_n \mid x_1, \ldots, x_{n-1}) $**：给定所有前面像素采样第n个像素

**网络架构（对于步骤 $n $）**：

- **输入**： $(n-1) $个像素值
- **输出**：第n个像素值的分布
- **架构**：可以是RNN、CNN或Transformer

**教师强制训练**：

- **相同条件结构**： $p(x_n \mid x_1, \ldots, x_{n-1}) $
- **不同输入**：使用真实标签像素而不是生成的像素
- **并行计算**：同时计算所有像素预测
- **目标偏移**：给定 $x_1, \ldots, x_i $预测 $x_{i+1} $

### 3.10 架构无关性

自回归公式是**架构无关的**：

**有效架构包括**：

- **RNN**：递归连接维护历史
- **CNN**：因果卷积尊重排序
- **Transformer**：因果注意力掩码防止未来依赖
- **自定义架构**：任何尊重条件结构的网络

**关键要求**：架构必须确保 $p(x_i \mid x_1, \ldots, x_{i-1}) $不依赖于 $j \geq i $的 $x_j $。

### 3.11 自回归模型总结

**核心原理**：

- **联合分布 → 条件概率的乘积**：通过链式法则精确分解
- **归纳偏差很重要**：共享架构、权重和排序假设
- **推理是自回归的**：使用模型自己的输出顺序生成
- **训练使用教师强制**：使用真实标签输入并行计算
- **架构灵活性**：可以用任何合适的网络设计实现

---

## 4. 自回归建模的网络架构

### 4.1 共享计算挑战

**朴素方法**需要 $n $个独立网络：

- 网络1：0个输入 → 预测 $x_1 $
- 网络2：1个输入（ $x_1 $） → 预测 $x_2 $
- 网络3：2个输入（ $x_1, x_2 $） → 预测 $x_3 $
- ...
- 网络 $n $： $(n-1) $个输入 → 预测 $x_n $

**问题**：

- 巨大的参数数量（不同网络大小）
- 跨位置没有共享学习
- 计算上令人望而却步

**问题**：我们能高效地用共享计算实现这个吗？

### 4.2 解决方案：共享计算

**关键洞察**：我们可以实现：

$$
p(x_1, \ldots, x_n) = p(x_1) \prod_{i=2}^{n} p(x_i \mid x_1, \ldots, x_{i-1})
$$

用**一个网络**具有：

- **共享架构**：所有位置使用相同网络结构
- **共享权重**：所有条件使用相同参数 $\theta $
- **共享计算**：重用中间计算

**关键约束**：输出 $x_i $不得依赖于任何 $j \geq i $的 $x_j $（因果性）

**训练效率**：

- **目标偏移一步**：输入序列 $[x_1, x_2, \ldots, x_{n-1}] $，目标序列 $[x_2, x_3, \ldots, x_n] $
- **并行计算**：所有位置同时预测
- **单次前向传播**：一次计算所有条件

### 4.3 常见架构概述

三个主要架构家族可以实现自回归建模：

1. **RNN（递归神经网络）**：带记忆的顺序处理
2. **CNN（卷积神经网络）**：因果卷积
3. **Attention（Transformer）**：因果自注意力

每种在计算效率、建模能力和实现复杂性方面有不同权衡。

### 4.4 递归神经网络（RNN）用于AR

**基本结构**：

- **RNN单元**：一次处理一个元素
- **在"时间"中展开**：在序列上创建计算图
- **深度网络**：堆叠多个RNN层

**数学公式**：

- **隐藏状态更新**： $h_t = \mathrm{RNN}(x_t, h_{t-1}) $
- **输出分布**： $p(x_{t+1} \mid x_1, \ldots, x_t) = \mathrm{softmax}(W_o h_t + b_o) $
- **信息流**：隐藏状态 $h_t $携带来自所有前面输入的信息

**训练过程**：

1. **输入序列**： $[x_1, x_2, \ldots, x_{n-1}] $
2. **目标序列**： $[x_2, x_3, \ldots, x_n] $（偏移一个）
3. **前向传播**：计算所有隐藏状态 $h_1, h_2, \ldots, h_{n-1} $
4. **损失**： $\mathcal{L} = -\sum_{t=1}^{n-1} \log p_\theta(x_{t+1} \mid h_t) $
5. **时间反向传播**：梯度通过展开的RNN反向流动

**条件映射**：

- $p(x_2 \mid x_1) $：使用 $h_1 = \mathrm{RNN}(x_1, h_0) $
- $p(x_3 \mid x_1, x_2) $：使用 $h_2 = \mathrm{RNN}(x_2, h_1) $
- $p(x_4 \mid x_1, x_2, x_3) $：使用 $h_3 = \mathrm{RNN}(x_3, h_2) $
- $p(x_5 \mid x_1, x_2, x_3, x_4) $：使用 $h_4 = \mathrm{RNN}(x_4, h_3) $

**示例：Char-RNN**

- **参考**：Andrej Karpathy "RNN的不合理有效性"（2015）
- **任务**：字符级文本生成
- **输入**：前面的字符
- **输出**：下一个字符分布
- **著名结果**：生成莎士比亚、代码、维基百科文章

### 4.5 卷积神经网络（CNN）用于AR

**关键概念：因果卷积**

**标准1-D卷积**： $y_i = \sum_{k=0}^{K-1} w_k \cdot x_{i+k} $

**因果卷积**： $y_i = \sum_{k=0}^{K-1} w_k \cdot x_{i-k} $（只向后看）

**实现步骤**：

1. **沿序列维度的1-D卷积**
2. **因果填充**：只填充左侧以保持因果性
3. **因果约束**：位置 $i $的输出只依赖于输入 $\leq i $
4. **深度网络**：堆叠多个因果卷积层

**性质**：

- **并行训练**：所有位置同时计算
- **顺序推理**：仍然需要一次生成一个
- **固定感受野**：每层看到有限上下文
- **长程依赖**：需要多层（或扩张卷积）

**训练过程**：

1. **输入**：填充序列 $[\mathrm{PAD}, x_1, x_2, \ldots, x_{n-1}] $
2. **因果卷积**：确保 $y_i $只使用 $j < i $的 $x_j $
3. **输出**：分布 $[p(x_1), p(x_2 \mid x_1), \ldots, p(x_n \mid x_1, \ldots, x_{n-1})] $

**条件映射**：

- $p(x_2 \mid x_1) $：位置2的卷积输出使用位置1的输入
- $p(x_3 \mid x_1, x_2) $：位置3的卷积输出使用输入1-2
- $p(x_4 \mid x_1, x_2, x_3) $：位置4的卷积输出使用输入1-3
- $p(x_5 \mid x_1, x_2, x_3, x_4) $：位置5的卷积输出使用输入1-4

**示例：WaveNet**

- **参考**：van den Oord等"WaveNet：原始音频的生成模型"（2016）
- **创新**：1-D扩张因果卷积
- **扩张卷积**：指数增长的感受野
- **应用**：高质量音频合成
- **架构**：扩张因果卷积层的深度堆叠

### 4.6 Transformer用于AR

**结构**：

- **自注意力机制**：每个位置关注所有前面位置
- **因果掩码**：防止关注未来位置
- **多层**：堆叠注意力层以增加容量

**因果注意力机制**：

1. **查询、键、值**: $Q = XW_Q $， $K = XW_K $， $V = XW_V$
2. **注意力分数**: $A_{i,j} = \frac{Q_i \cdot K_j}{\sqrt{d_k}}$
3. **因果掩码**: 对 $j > i $设置 $A_{i,j} = -\infty$
4. **Softmax**: $\mathrm{Attn}\_{i,j} = \frac{\exp(A\_{i,j})}{\sum_{k \leq i} \exp(A_{i,k})}$
5. **输出**: $O_i = \sum_{j \leq i} \mathrm{Attn}_{i,j} V_j$

**掩码模式**：

- 位置1：只能关注位置1
- 位置2：可以关注位置1-2
- 位置3：可以关注位置1-3
- 位置 $i $：可以关注位置 $1, \ldots, i $

**训练过程**：

1. **输入序列**： $[x_1, x_2, \ldots, x_{n-1}] $
2. **目标序列**： $[x_2, x_3, \ldots, x_n] $（偏移）
3. **因果注意力**：带掩码的并行计算
4. **所有预测**：在单次前向传播中同时计算

**性质**：

- **$O(n^2)$复杂度**：序列长度的二次方
- **直接连接**：每个位置可以直接关注所有前面位置
- **最强大**：最适合捕获长程依赖
- **并行训练**：所有位置同时计算
- **灵活**：容易实现不同注意力模式

**条件映射**：

- $p(x_2 \mid x_1) $：位置2只关注位置1
- $p(x_3 \mid x_1, x_2) $：位置3关注位置1-2
- $p(x_4 \mid x_1, x_2, x_3) $：位置4关注位置1-3
- $p(x_5 \mid x_1, x_2, x_3, x_4) $：位置5关注位置1-4

**示例：image GPT（iGPT）**

- **参考**：Chen等"来自像素的生成预训练"（ICML 2020）
- **方法**：将图像视为像素序列
- **架构**：带因果注意力的Transformer
- **训练**：图像的无监督预训练
- **应用**：图像生成、表示学习
- **创新**：证明Transformer在NLP之外的有效性

### 4.7 架构比较

| **方面**       | **RNN**        | **CNN**      | **Transformer** |
| -------------------- | -------------------- | ------------------ | --------------------- |
| **训练**       | 顺序                 | 并行               | 并行                  |
| **推理**       | 顺序                 | 顺序               | 顺序                  |
| **长程依赖**   | 通过隐藏状态         | 需要多层           | 直接连接              |
| **计算复杂度** | $O(n) $          | $O(n) $        | $O(n^2) $         |
| **每步内存**   | $O(1) $          | $O(1) $        | $O(n) $           |
| **并行性**     | 有限                 | 高                 | 高                    |
| **梯度流**     | 可能消失（长链累积） | 局部传播（需多跳） | 单步直达（无衰减）    |
| **可解释性**   | 隐藏状态             | 局部滤波器         | 注意力权重            |

**权衡**：

- **RNN**：内存高效但训练慢，潜在梯度问题
- **CNN**：快速且可并行但感受野有限
- **Transformer**：最强大但计算成本最高

### 4.8 统一框架

所有三种架构都实现相同的数学框架：

$$
p(x_1, \ldots, x_n) = p(x_1) \prod_{i=2}^{n} p(x_i \mid x_1, \ldots, x_{i-1})
$$

**它们的区别在于**：

- **信息聚合**：如何组合来自 $x_1, \ldots, x_{i-1} $的上下文
- **计算效率**：训练和推理速度
- **建模能力**：捕获复杂依赖的能力
- **归纳偏差**：对数据结构的内置假设

**共同性质**：

- **因果性**：都遵循自回归约束
- **教师强制**：都可以用并行计算训练
- **精确似然**：都使能精确概率计算

---

## 5. 实例分析

### 5.1 示例1：MNIST像素生成

**设置**：

- **输入**：28×28灰度图像（784像素）
- **序列**：光栅扫描顺序（从左到右，从上到下）
- **建模**： $p(x_1, x_2, \ldots, x_{784}) = p(x_1) \prod_{i=2}^{784} p(x_i \mid x_1, \ldots, x_{i-1}) $

**架构选项**：

- **RNN**：隐藏状态累积空间上下文
  - 通常将2D展平成1D序列处理
  - 按光栅扫描顺序逐像素处理
- **CNN**：2D因果卷积（掩码未来像素）
  - **天然处理2D结构**：保留空间邻域关系
  - 使用掩码卷积核：只看左上方和左侧像素
  - 例如：PixelCNN使用特殊的掩码卷积
- **Transformer**：可以采用两种方式
  - **1D序列方式**（更常见）：将28×28展平成784长度序列，加1D位置编码
  - **2D方式**：保持2D结构，使用2D位置编码和2D因果掩码（如Image Transformer）
  - 1D方式更简单但丢失空间结构；2D方式更自然但实现复杂

**挑战**：

- 长程空间依赖
- 像素值是连续的[0,1]或离散的[0,255]
- 排序很重要：光栅vs螺旋vs随机排序给出不同结果

### 5.2 示例2：Char-RNN

**任务**：字符级语言建模

**架构**：

- **输入**：onehot编码字符
- **RNN**：带隐藏状态的LSTM或GRU
- **输出**：词汇表上的softmax（所有可能字符）

**训练**：

- **序列**："hello world" → 输入："hello worl"，目标："ello world"
- **损失**：每个位置的交叉熵
- **教师强制**：使用真实标签字符作为输入

**生成**：

- 从种子字符开始
- 从预测分布中采样下一个字符
- 将采样字符作为下一次预测的输入
- 重复直到所需长度

**著名结果**：

- 生成莎士比亚风格文本
- 具有正确语法的现实C代码
- 具有正确格式的LaTeX文档

### 5.3 示例3：WaveNet

**任务**：原始音频生成

**关键创新**：

- **1-D扩张因果卷积**：指数增长的感受野
- **门控激活函数**： $\tanh(W_f \star x) \odot \sigma(W_g \star x) $
- **跳跃连接**：密集连接模式
- **量化音频**：8位μ-law编码（256类）

**架构**：

- 扩张因果卷积层堆栈
- 扩张因子：1, 2, 4, 8, 16, ..., 256, 1, 2, 4, ...（重复块）
- 每个样本都以感受野中所有前面样本为条件

**应用**：

- 文本到语音合成（带条件）
- 音乐生成
- 语音增强
- 通用音频建模

**影响**：相比之前方法在音频质量上的革命性改进

### 5.4 示例4：image GPT（iGPT）

**方法**：将GPT架构直接应用于图像

**预处理**：

- 将图像调整为32×32、64×64或128×128
- 展平为1D序列：32×32×3 = 3072个标记
- 减少调色板（9位颜色 → 512个可能值）

**架构**：

- **标准Transformer**：多头自注意力 + MLP
- **因果掩码**：位置 $i $只关注位置 $< i $
- **大规模**：高达68亿参数

**训练**：

- **无监督**：无标签，只是像素预测
- **自回归**：给定所有前面像素预测下一个像素
- **大数据集**：ImageNet + 额外网络图像

**应用**：

- **生成**：采样高质量图像
- **表示学习**：特征迁移到下游任务
- **微调**：分类、检测、分割

**意义**：

- 证明Transformer适用于视觉（在Vision Transformer之前）
- 展示图像生成预训练的力量
- 在某些任务上与监督预训练竞争

---

## 6. 总结和关键要点

### 6.1 主要概念

**1. 条件分布建模**

- **基础**：概率的链式法则
- **灵活性**：任何排序、任何分区在数学上都有效
- **依赖图**：分解选择编码假设
- **神经实现**：每个条件由网络建模

**2. 自回归模型**

- **核心思想**：联合分布 → 条件概率的乘积
- **关键洞察**：使用自己的输出进行顺序生成
- **训练策略**：并行计算的教师强制
- **归纳偏差**：共享架构、权重和排序

**3. 网络架构**

- **RNN**：带隐藏状态记忆的顺序处理
- **CNN**：并行训练的因果卷积
- **Transformer**：直接长程连接的因果注意力
- **统一框架**：都实现相同数学原理

### 6.2 关键设计选择

**排序决定**：

- **对复杂性的影响**：某些排序更容易学习
- **领域知识**：自然排序（时间、空间、因果性）
- **数据驱动**：从数据学习最优排序（MAR模型）

**架构选择**：

- **RNN**：最适合固有顺序数据，内存高效
- **CNN**：效率和建模能力的良好平衡
- **Transformer**：最强大但计算昂贵

**训练考虑**：

- **教师强制vs计划采样**：平衡训练稳定性和推理真实性
- **序列长度**：更长序列更具挑战性
- **计算预算**：架构选择影响可扩展性

### 6.3 自回归模型的优势

**数学优势**：

- **精确似然**：无近似或界限
- **灵活**：可以建模任何分布（通用近似）
- **可解释**：清晰的概率解释

**实际优势**：

- **稳定训练**：理解良好的优化
- **质量**：跨领域的最先进结果
- **可控**：易于以上下文为条件

**成功应用案例**：

- **语言领域**：GPT系列/BERT解码器/T5
- **视觉领域**：PixelRNN/PixelCNN/iGPT
- **音频领域**：WaveNet/Tacotron/MusicLM
- **多模态领域**：DALL-E/Flamingo/GPT-4o

### 6.4 挑战和限制

**计算挑战**：

- **顺序生成**：慢推理（特别是长序列）
- **二次复杂度**：Transformer随长度扩展困难
- **内存**：长序列需要大量内存

**训练问题**：

- **暴露偏差**：由于教师强制导致的训练/推理不匹配
- **分布偏移**：模型在训练期间从未看到自己的错误
- **排序敏感性**：性能很大程度上依赖于选择的排序

**建模限制**：

- **单向**：信息仅在一个方向流动
- **固定排序**：传统AR模型使用单一固定排序
- **局部依赖**：某些架构在非常长程依赖上有困难

### 6.5 现代发展和未来方向

**架构改进**：

- **高效注意力**：线性注意力、稀疏模式、检索增强
- **专家混合**：在不按比例增加计算的情况下扩展模型能力
- **混合模型**：结合AR与其他范式（扩散、VAE）

**训练创新**：

- **课程学习**：从短序列开始，逐渐增加
- **计划采样**：从教师强制逐渐过渡到自回归
- **强化学习**：针对下游任务性能优化

**建模进展**：

- **排序混合**：MAR模型同时使用多个排序
- **双向**：结合前向和后向AR模型
- **非自回归**：带迭代细化的并行生成

## 参考文献

### 主要参考文献：

1. **Bengio and Bengio**. "Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks", NeurIPS 1999

   - 神经自回归模型的基础论文
   - 介绍关键概念和NADE架构
2. **van den Oord et al.** "Pixel Recurrent Neural Networks", ICML 2016

   - 将RNN应用于图像生成的里程碑论文
   - 证明AR模型可以生成高质量图像
3. **Radford et al.** "Improving Language Understanding by Generative Pre-Training", 2018

   - 原始GPT论文，展示AR预训练的力量
   - 建立了基于Transformer的语言建模范式

### 额外参考文献：

4. **Andrej Karpathy**. "The Unreasonable Effectiveness of Recurrent Neural Networks", blog post, 2015

   - 展示Char-RNN能力的有影响力博客文章
   - 基于RNN文本生成的可访问介绍
5. **van den Oord et al.** "WaveNet: A Generative Model for Raw Audio", 2016

   - 革命性的基于CNN的音频生成
   - 介绍扩张因果卷积
6. **Chen et al.** "Generative Pretraining from Pixels", ICML 2020

   - 将GPT架构应用于图像（iGPT）
   - 展示生成预训练适用于视觉
7. **Hua et al.** "Self-supervision through Random Segments with Autoregressive Coding (RandSAC)", ICLR 2022

   - AR模型中排序的现代方法
   - 展示排序选择的重要性
8. **Li et al.** "Autoregressive Image Generation without Vector Quantization", 2024

   - AR图像建模的最新进展
   - 具有灵活排序的掩码自回归（MAR）模型

---

## 附录：数学基础

### A.1 概率的链式法则

对于 $n $个变量的任意联合分布：

$$
p(x_1, x_2, \ldots, x_n) = p(x_1) \prod_{i=2}^{n} p(x_i \mid x_1, \ldots, x_{i-1})
$$

这种分解是**精确的**，不是近似。它对变量的任何排序都成立。

### A.2 对数似然目标

自回归模型下序列的对数似然：

$$
\log p(x_1, \ldots, x_n) = \log p(x_1) + \sum_{i=2}^{n} \log p(x_i \mid x_1, \ldots, x_{i-1})
$$

这是训练期间最大化的目标函数。

### A.3 交叉熵损失

对于离散自回归模型，训练损失是：

$$
\mathcal{L} = -\sum_{i=1}^{n} \log p_\theta(x_i \mid x_1, \ldots, x_{i-1})
$$

每个项 $-\log p_\theta(x_i \mid x_1, \ldots, x_{i-1}) $是标准分类损失（交叉熵），其中：

- **真实类别**： $x_i $（真实标签下一个标记）
- **预测分布**： $p_\theta(\cdot \mid x_1, \ldots, x_{i-1}) $

### A.4 教师强制与自回归损失

**教师强制（训练）**：

$$
\mathcal{L}_{TF} = -\sum_{i=2}^{n} \log p_\theta(x_i^{true} \mid x_1^{true}, \ldots, x_{i-1}^{true})
$$

**自回归（推理）**：

$$
\mathcal{L}_{AR} = -\sum_{i=2}^{n} \log p_\theta(x_i^{true} \mid x_1^{sample}, \ldots, x_{i-1}^{sample})
$$

这两个损失之间的不匹配导致暴露偏差。

---

**为MIT 6.S978深度生成模型，2024年秋季创建的文档**
**第3讲：自回归模型**
**授课教师：Kaiming He**