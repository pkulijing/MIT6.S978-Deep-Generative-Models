# [MIT 6.S978 Deep Generative Models](https://mit-6s978.github.io/schedule.html)

## 课程笔记与详细解析 (Fall 2024)

## 阅读材料 1：图像先验 Image Priors

> 📚 **阅读材料对应课程第 2 周，在 Lecture 2 (VAE)之前学习**

本章深入分析 Week 2 的三篇核心阅读论文，它们从不同角度探讨了图像先验（Image Prior）的本质：什么样的统计特性使自然图像区别于随机噪声？如何建模这些先验并应用于图像恢复任务？

### 📄 论文 1：From Learning Models of Natural Image Patches to Whole Image Restoration

**Daniel Zoran, Yair Weiss | ICCV 2011 | 引用次数: 1895+**

> 📄 **论文下载：**
>
> - [MIT CSAIL PDF](https://people.csail.mit.edu/danielzoran/EPLLICCVCameraReady.pdf)
> - [IEEE Xplore](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6126278)

#### 论文主旨

这篇开创性论文探讨了**图像先验（Image Prior）**的核心问题：如何从图像块（patches）的先验模型扩展到完整图像的复原。论文提出了**EPLL**（Expected Patch Log Likelihood）框架，这是一个通用的图像复原框架。

#### 三个核心问题

论文围绕三个关键问题展开：

1. **问题 1：** 对数据给出高似然的块先验是否在块复原中表现更好？
2. **问题 2：** 对数据给出高似然的块先验是否在整图复原中表现更好？
3. **问题 3：** 我们能否学习更好的块先验？

#### 关键发现

**1. 似然与复原性能的关系（问题 1）**

论文比较了多种 patch 模型：

- **独立像素模型**（Ind. Pixel）：每个像素独立建模，学习边缘分布
- **多元高斯**（MVG）：学习像素间的协方差
- **PCA**：独立主成分，学习非高斯边缘分布
- **ICA**：学习独立成分分析的边缘分布

**实验结论**：模型对数据的似然越高，在 patch 去噪任务中的 PSNR 越高。这个发现为后续工作提供了重要指导。

**2. EPLL 框架（问题 2）**

传统方法的问题：

- **非重叠块复原**：在块边界产生明显伪影
- **简单平均**：虽然减少伪影，但重建图像中的 patch 在先验下不一定具有高似然

**EPLL 的核心思想**：

定义 Expected Patch Log Likelihood：

$$
\text{EPLL}_p(x) = \sum_i \log p(P_i x)
$$

其中 $P_i$ 提取第 $i$ 个重叠 patch。EPLL 是图像中随机选择的 patch 的期望对数似然。

**优化目标**：

$$
f_p(x|y) = \frac{\lambda}{2}\|Ax - y\|^2 - \text{EPLL}_p(x)
$$

其中：

- 第一项是**数据保真项**（data fidelity），确保复原图像接近观测
- 第二项是**EPLL 先验项**，确保复原图像中的 patches 具有高似然
- $A$ 是退化矩阵（去噪时为单位矩阵，去模糊时为卷积矩阵，修复时为对角 mask 矩阵）

**Half-Quadratic Splitting 优化**：

引入辅助变量 $\{z_i\}$，将问题分解为两个子问题交替优化：

1. **固定 $\{z_i\}$，求解 $x$**：这是一个闭式解

$$
\hat{x} = \left(\lambda A^T A + \beta\sum_j P_j^T P_j\right)^{-1}\left(\lambda A^T y + \beta\sum_j P_j^T z_j\right)
$$

2. **固定 $x$，求解 $\{z_i\}$**：对每个 patch 求 MAP 估计

$$
z_i = \arg\max_z p(z) \cdot \exp\left(-\frac{\beta}{2}\|P_i x - z\|^2\right)
$$

通过逐渐增大 $\beta$，强制 $z_i$ 收敛到 $P_i x$。

**3. 高斯混合模型先验（问题 3）**

论文提出使用**GMM**（Gaussian Mixture Model）作为 patch 先验：

$$
p(x) = \sum_k \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)
$$

**关键特点**：

- 学习**完整的协方差矩阵**，不做任何约束
- 使用 EM 算法学习 200 个混合成分
- 从 $2\times 10^6$ 个自然图像 patches 中学习

**GMM 的优势理解**：

零均值 GMM 可以理解为一种**结构化稀疏表示**：

- 每个样本 $x$ 可以被其所属混合成分的前 $m$ 个特征向量很好地近似
- 所有混合成分的特征向量构成一个"字典"
- 每个样本只使用字典中 $1/200$ 的元素（非常稀疏）
- 但与 ICA/稀疏编码不同，**只有同一成分的字典元素可以共同激活**

**学到的结构**：

GMM 的协方差矩阵特征向量展现出丰富的结构：

- 一些类似 PCA 成分
- 一些建模**遮挡**（occlusions）
- 一些建模**纹理边界和边缘**
- 与稀疏编码学到的 Gabor 滤波器**非常不同**

#### 实验结果

**性能对比（$\sigma=25$ 的去噪）**：

- **Ind. Pixel**: 25.26 dB (EPLL)
- **MVG**: 27.71 dB (EPLL)
- **PCA**: 29.42 dB (EPLL)
- **ICA**: 29.53 dB (EPLL)
- **GMM**: **29.85 dB (EPLL)** ← 最佳

**与其他方法对比**：

| $\sigma$ | KSVD(Generic) | FoE   | GMM-EPLL  | BM3D  |
| -------- | ------------- | ----- | --------- | ----- |
| 25       | 28.28         | 27.77 | **28.71** | 28.57 |
| 50       | 25.18         | 23.29 | **25.72** | 25.63 |

GMM-EPLL 在通用先验中达到 state-of-the-art，甚至与基于图像特定的方法（如 BM3D）**竞争性强**。

#### 与课程的关系

这篇论文对理解生成模型至关重要：

1. **先验的重要性**：展示了学习良好的数据先验对于生成和复原任务的关键作用
2. **Patch vs. Image**：说明了为什么在 patch 级别学习模型更容易，但需要巧妙的方法扩展到整图
3. **GMM 的威力**：GMM 是 VAE 的理论基础之一，这篇论文展示了即使是简单的 GMM 也能捕捉复杂的图像统计特性
4. **ELBO 的前身**：EPLL 框架中的优化思想与 VAE 中的 ELBO 优化有相似之处
5. **现代方法的基础**：这种 patch-based 的思想影响了后续的 VQ-VAE/VQGAN 等 tokenizer 设计

#### 核心贡献总结

- ✓ 证明了似然与复原性能的正相关
- ✓ 提出 EPLL 通用框架，可插入任何 patch 先验
- ✓ 展示 GMM 在 image prior 建模中的惊人表现
- ✓ 桥接了学习方法（深度网络）和无学习方法（手工先验）

---

### 📄 论文 2：Natural Images, Gaussian Mixtures and Dead Leaves

**Daniel Zoran, Yair Weiss | NeurIPS 2012**

> 📄 **论文下载：**
>
> - [NeurIPS PDF](https://proceedings.neurips.cc/paper_files/paper/2012/file/e97ee2054defb209c35fe4dc94599061-Paper.pdf)
> - [Supplementary Material](https://people.csail.mit.edu/danielzoran/GMMsupp.pdf)

#### 论文主旨

这篇论文是上一篇的延续，深入分析了**GMM 究竟从自然图像中学到了什么**，以及为什么 GMM 能如此有效地建模自然图像 patches。

#### 核心研究问题

论文系统研究了 GMM 随混合成分数量 K 变化的行为：

- 协方差结构如何变化？
- 对比度（contrast）统计如何变化？
- 学到的结构与**Dead Leaves 模型**有何关系？

#### 关键发现

**1. 多尺度结构的涌现**

随着 K 增大，GMM 的混合成分捕捉不同尺度的结构：

- **K 较小时**：主要捕捉低频、大尺度的变化
- **K 增大时**：逐渐捕捉更精细的纹理和边缘
- **K 很大时**：能够捕捉非常精细的局部结构

**2. Dead Leaves 模型的联系**

**Dead Leaves 模型**是一个经典的图像生成模型：

- 随机放置不同尺度的"叶子"（圆盘）
- 后放置的叶子遮挡先放置的
- 产生的图像统计特性与自然图像相似

论文发现：**从 Dead Leaves 模型生成的图像训练的 GMM，与从自然图像训练的 GMM，学到了相似的结构**。这表明：

- 自然图像的复杂统计特性可能来源于简单的遮挡过程
- GMM 能够隐式地学习到这种遮挡结构

**3. 对比度统计**

论文分析了 GMM 如何建模 patch 的对比度（contrast）：

- 自然图像 patches 的对比度分布是**重尾的**（heavy-tailed）
- GMM 通过使用不同尺度的协方差矩阵来捕捉这种重尾分布
- 这解释了为什么 GMM 比单一的高斯或拉普拉斯先验更有效

#### 理论洞察

论文提供了对**为什么简单的 GMM 如此有效**的深刻理解：

1. **分层建模**：GMM 隐式地进行了分层建模——每个成分捕捉一种"类型"的 patch
2. **稀疏性**：虽然 GMM 不显式强制稀疏性，但其结构暗示了一种稀疏表示
3. **尺度不变性**：GMM 能够捕捉自然图像在不同尺度上的统计规律

#### 与课程的关系

- **VAE 的启发**：GMM 可以看作离散的隐变量模型，启发了 VAE 的设计
- **生成过程**：从 GMM 采样的过程（先选择成分，再从该成分采样）类似于 VAE 的生成过程
- **层次结构**：说明了为什么层次化、多尺度的表示对于建模复杂数据很重要

---

### 📄 论文 3：Deep Image Prior

**Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky | CVPR 2018 | 引用次数: 3000+**

> 📄 **论文下载：**
>
> - [arXiv PDF](https://arxiv.org/pdf/1711.10925)
> - [CVPR 2018 Open Access](https://openaccess.thecvf.com/content_cvpr_2018/papers/Ulyanov_Deep_Image_Prior_CVPR_2018_paper.pdf)
> - [GitHub Code](https://github.com/DmitryUlyanov/deep-image-prior)

#### 论文主旨

这篇**颠覆性**的论文提出了一个惊人的发现：**随机初始化的神经网络本身就包含了强大的图像先验**，无需在大量数据上训练！

#### 核心发现：网络结构 = 先验

**传统观念**：深度网络的优异表现来自于从大量样本中学习到的真实图像先验。

**Deep Image Prior 的发现**：**生成器网络的结构本身就足以捕捉大量低层图像统计特性**，无需任何学习！

#### 方法

**设置**：

- 使用**随机初始化**的卷积神经网络（通常是 U-Net 或 ResNet 架构）
- 输入：固定的随机噪声 $z$
- 输出：重建的图像 $x = f_\theta(z)$
- 优化目标：仅拟合**单张**损坏的图像

$$
\theta^* = \arg\min_\theta L(f_\theta(z), x_{\text{corrupted}})
$$

**关键点**：

- 不使用任何训练数据集
- 网络参数 $\theta$ 是**随机初始化**的
- 优化过程中只看到**一张**损坏的图像

#### 为什么有效？

论文提出的解释：

1. **归纳偏置**：

   - 卷积结构强制**平移不变性**
   - 多尺度的编码器-解码器结构提供**尺度协变性**
   - 跳跃连接保留**局部统计特性**

2. **低频偏置**：

   - 深度网络倾向于先拟合**低频（平滑）成分**
   - 需要更多迭代才能拟合高频噪声
   - 这提供了一种**隐式正则化**

3. **参数化方式**：

   - 通过网络映射 z→x 限制了可表达的图像空间
   - 这个空间更接近自然图像 manifold

#### 应用

论文展示 Deep Image Prior 在多个任务中的惊人表现：

**1. 去噪（Denoising）**

- 仅优化拟合噪声图像
- 早停（early stopping）避免过拟合噪声
- 性能接近甚至超过传统方法

**2. 超分辨率（Super-resolution）**

- 输入：下采样的低分辨率图像
- 输出：高分辨率重建
- 网络结构偏好平滑、自然的高频细节

**3. 修复（Inpainting）**

- 损失只在可见像素上计算
- 网络自动"幻想"出合理的缺失区域
- 效果令人惊讶地好

**4. Flash-no-flash 重建**

- 使用闪光灯和无闪光灯图像对
- 保留无闪光图像的颜色，闪光图像的细节

#### 关键实验

**实验 1：不同网络架构的比较**

- **U-Net**: 表现最好，平衡了全局和局部信息
- **ResNet**: 也很有效
- **全连接网络**: 表现较差，缺乏卷积的归纳偏置

**实验 2：训练 vs. 未训练网络**

比较：

- **Deep Image Prior**（未训练）
- **预训练生成器**（如 GAN）

发现：在某些任务上，未训练的网络**表现更好**！原因：预训练网络可能过拟合训练数据的分布。

**实验 3：优化过程的分析**

观察网络在优化过程中的行为：

- **早期**：快速捕捉低频结构
- **中期**：逐渐细化纹理和细节
- **后期**：开始拟合噪声（过拟合）

这解释了为什么**早停**至关重要。

#### 理论意义

**1. 重新理解深度学习**

Deep Image Prior 挑战了传统观念：

- 深度学习的成功不仅仅来自于"从数据中学习"
- **网络架构本身编码了强大的先验**
- 这个先验可能比我们想象的更重要

**2. 桥接学习与无学习方法**

论文展示了：

- **基于学习的方法**（深度网络）
- **无学习方法**（手工先验，如 total variation）

实际上存在连续统：**网络结构 = 手工设计的先验**

**3. 对生成模型的启示**

- **结构化先验**的重要性（如卷积、跳跃连接）
- 为什么某些架构（U-Net）在生成任务中特别有效
- 训练 vs.推理时的不同行为

#### 与课程的关系

Deep Image Prior 对理解生成模型至关重要：

1. **与 Diffusion 模型的联系**：

   - Diffusion 模型中的 U-Net 去噪器也利用了网络结构的先验
   - Deep Image Prior 的成功部分解释了为什么 U-Net 在 diffusion 中如此有效

2. **与 VAE 的联系**：

   - VAE 的解码器也在学习一个从潜在空间到图像空间的映射
   - 解码器的架构选择至关重要

3. **对 GAN 的启示**：

   - 生成器的架构设计不仅仅是"容量"问题
   - 正确的归纳偏置可以大大减少所需的训练数据

4. **归纳偏置的重要性**：

   - 深度生成模型 = **设计的结构**（归纳偏置）+ **学习的参数**
   - 这与 Lecture 1 中强调的"not all is learned"一致

#### 批判性思考

**优点**：

- ✓ 无需大规模数据集
- ✓ 可解释性强
- ✓ 展示了架构的重要性

**局限性**：

- ✗ 需要为每张图像单独优化（计算昂贵）
- ✗ 需要仔细调整早停时间
- ✗ 对某些任务（如生成新样本）不适用

#### 后续影响

Deep Image Prior 启发了大量后续工作：

- **压缩感知**：使用 DIP 作为先验
- **逆问题求解**：医学成像、计算摄影
- **神经架构搜索**：寻找更好的归纳偏置
- **可解释 AI**：理解网络"知道"什么

---

### 💡 Week 2 Reading 总结

> 这三篇论文共同构建了对**图像先验**的深刻理解：

1. **Zoran & Weiss (2011)**: 展示了从 patch 先验到整图复原的框架，以及 GMM 的强大能力
2. **Zoran & Weiss (2012)**: 深入分析 GMM 学到的结构，揭示自然图像统计的本质
3. **Ulyanov et al. (2018)**: 颠覆性地发现网络结构本身就是先验

**统一的主题**：**先验在生成模型中的核心作用**——无论是显式学习的（GMM）还是隐式编码在架构中的（Deep Image Prior）。

**与 VAE 的联系**：这些工作为理解 VAE 提供了重要基础——VAE 本质上也在学习数据的先验分布，只是通过不同的方式（隐变量模型）。
